{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "  \n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provincial Achievement Test Scores\n",
    "\n",
    "## Introduction\n",
    "\n",
    "TODO: Improve discussion and add quantitative results if there's interest in this notebook to do that. \n",
    "\n",
    "Every year, the province of Alberta runs standardized testing for grades 6 and 9 for primary courses under the blanket identifier of Provincial Achievement Tests in order to assess how well the students preform. The results of these test are open source and readily downloaded from the Alberta Education website. In this notebook we're going to download and manipulate the data direct from Alberta education, and see if we can easily identify under and over performing school districts. Time permitting, we might even toss these onto a map using another open data set from Alberta education which contains the addresses of every school in Alberta. Using this data in combination with the provincial testing scores, we will likely be able to easily identify which school districts/schools are performing best and worst.  \n",
    "\n",
    "## Wrangling the data\n",
    "\n",
    "First let's download the data directly from the Alberta Education website and toss it in a Pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import nan as Nan\n",
    "\n",
    "\n",
    "df_ero = pd.read_excel(\"https://education.alberta.ca/media/3680591/pat-multiyear-sch-list.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was pretty easily done as those are hosted excel spreadsheets. So, we don't even have to save the file locally, we can toss it straight in a pandas frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_results = df_ero.copy()\n",
    "school_results[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the above data format is going to be annoying to work with in order to plot/sort some data. Instead, let's whip this data table into \"long form\" so that we can manipulate, analyze and plot this data more easily. We do this with the code below. Notice how now we have multiple duplicate entries for \"Authority Name\" and \"School Name\" columns, as well as a handy year column for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "schools_reshaped = school_results.copy()\n",
    "\n",
    "# If there's a year in the column we want to split that bad boy\n",
    "def splitter(string):\n",
    "    r = re.compile(r'\\d{4}|\\S.*$')\n",
    "    return r.findall(string)\n",
    "\n",
    "cols = list(schools_reshaped)[0:8]\n",
    "years = ['2013', '2014', '2015', '2016', '2017']\n",
    "\n",
    "# Being lazy and creating duplicate columns with a year index. It's the same \n",
    "# accross the board but we need them for the next step. \n",
    "# The key is to not respect your RAM. \n",
    "for year in years:\n",
    "    for names in cols:\n",
    "        schools_reshaped[str(year +\" \"+names)] = schools_reshaped[names]\n",
    "\n",
    "\n",
    "schools_reshaped.columns = pd.MultiIndex.from_tuples([tuple(splitter(c)) for c in schools_reshaped.columns])\n",
    "schools_reshaped = schools_reshaped.stack(0).reset_index(1)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "schools_reshaped.rename(columns={'level_1': \"Year\"}, inplace=True)\n",
    "#schools_reshaped[[\"School Name\", \"Course Name\", \"Sch Enrol\", \"Year\", \"Sch Writing\"]].loc[schools_reshaped['Year'] == '2013']\n",
    "\n",
    "# Sort by school name. \n",
    "schools_reshaped=schools_reshaped.sort_values('School Name')\n",
    "\n",
    "schools_reshaped[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. Now that the data have been reshaped into a \"long form\" they'll be a lot easier to work with when it comes to plotting and analysis. So, let's start to get an idea at the score distributions between schools and districts by using this dataframe as a back end to an interactive widget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Graph\n",
    "\n",
    "Before we start any more \"involved\" analysis let's take a moment to plot these data by year to get an idea of what we're working with. In the widget below `_type` controls whether we're looking at individual schools or the school authority, `name` is the name of the school/authority, `subject` changes the subject, and `name2` is optional and will display another school/authority to compare with. Note that switching to school is a little slower, as that data set requires some set up before we can put it nicely into the widget. Also note that not all subjects are offered in each school, and they're filtered down buy what subjects were offered in the school/authority under `name`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact\n",
    "init_notebook_mode(connected=True)\n",
    "from ipywidgets import Dropdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do the same with school districts\n",
    "# print(list(schools_reshaped))\n",
    "\n",
    "def traces(name, subject, school_or_auth):\n",
    "    result = None\n",
    "    divisor = None\n",
    "    y = None\n",
    "    y2 = None\n",
    "    y3 = None\n",
    "    if school_or_auth == \"Authority Name\":\n",
    "        result = schools_reshaped[schools_reshaped[school_or_auth] == name]\n",
    "        divisor = result.groupby(\"Year\")['Sch Enrol'].sum() - result.groupby(\"Year\")[\"Sch Absent\"].sum()    \n",
    "        y =  100 * (result.groupby(\"Year\")[\"Sch Acc\"].sum() - result.groupby(\"Year\")[\"Sch Exc\"].sum() )/divisor\n",
    "        y2 = 100 * result.groupby(\"Year\")['Sch Exc'].sum()/divisor\n",
    "        y3 = 100 * result.groupby(\"Year\")['Sch Below'].sum()/divisor\n",
    "    \n",
    "    if school_or_auth == \"School Name\":\n",
    "        result = schools_reshaped[schools_reshaped[school_or_auth] == name]\n",
    "        divisor = result['Sch Writing']\n",
    "        y = (result['Sch % Acc of Writing']-result['Sch % Exc of Writing'])# - result['Sch Exc']) / divisor\n",
    "        y2 = result['Sch % Exc of Writing'] #/ divisor\n",
    "        y3 =  result['Sch % Below of Writing']# / divisor \n",
    "        \n",
    "    result = result[result['Course Name'] == subject]\n",
    "\n",
    "    trace1 = go.Bar(x=result['Year'], y=y,\n",
    "                    name=\" \".join([name, '% at or above acceptable standard']))#, \n",
    "               \n",
    "    trace2 = go.Bar(x=result['Year'],\n",
    "                    y=y2,\n",
    "                    name= \" \".join([name, '% achieved a standard of excellence']))#,\n",
    "                \n",
    "    trace3 = go.Bar(x=result['Year'], \n",
    "                    y=y3, \n",
    "                    name = \" \".join([name,\"% below acceptable standard\"]))#,\n",
    "     \n",
    "    return [trace1, trace2, trace3]\n",
    "\n",
    "\n",
    "\n",
    "def compare_results( _type, name, subject, name2 = []):\n",
    "    \n",
    "    print(name, subject, _type)\n",
    "    \n",
    "    data = traces(name, subject, _type)\n",
    "    \n",
    "    if name2: \n",
    "        data2 = traces(name2, subject, _type)\n",
    "        data = data + data2\n",
    "    \n",
    "    layout = go.Layout(title=subject,\n",
    "                xaxis=dict(title='Year'),\n",
    "                yaxis=dict(title='Percentage',\n",
    "                      range = [0,100])\n",
    "                      )\n",
    "    \n",
    " \n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig)\n",
    "\n",
    "    \n",
    "def course_drop(_type, name):\n",
    "    courses = list(schools_reshaped['Course Name'].unique())\n",
    "    filtered_course_list = []\n",
    "   \n",
    "    for course in courses:\n",
    "        result = schools_reshaped[schools_reshaped[_type] == name]\n",
    "        result = result[result['Course Name'] == course]\n",
    "        if _type == \"School Name\":\n",
    "            y = result['Sch % Acc of Writing']\n",
    "       \n",
    "        if _type == \"Authority Name\":\n",
    "           y = result.groupby(\"Year\")[\"Sch Acc\"].sum() - result.groupby(\"Year\")[\"Sch Exc\"].sum()\n",
    "            \n",
    "        if y.isnull().sum() > 4 or y.empty == True:\n",
    "            # No course for school, do nothing\n",
    "            #filtered_course_list.append(course)\n",
    "           \n",
    "            pass\n",
    "        else:\n",
    "            # if something exists, we'll count ita\n",
    "            filtered_course_list.append(course)\n",
    "            \n",
    "    if len(filtered_course_list) == 0:\n",
    "        # TODO: make an empty thing instead of pretending they do math\n",
    "        filtered_course_list.append(\"Mathematics 6\")\n",
    "    return filtered_course_list\n",
    "\n",
    "course_widget = Dropdown()\n",
    "\n",
    "type_widget = Dropdown(options = [\"School Name\", \"Authority Name\"], value = \"School Name\")\n",
    "\n",
    "name_widget = Dropdown()\n",
    "name_widget2 = Dropdown()\n",
    "\n",
    "    \n",
    "def update2(*args):\n",
    "    a = sorted(list(map(str, list(schools_reshaped[type_widget.value].unique()))))\n",
    "    name_widget.options = a\n",
    "    name_widget2.options =  a\n",
    "    name_widget2.value = None\n",
    "    # course_widget.options = course_drop(type_widget.value, x_widget.value)\n",
    "    name_widget.value = a[0]\n",
    "    \n",
    "\n",
    "    \n",
    "def update(*args):\n",
    "    course_widget.options = course_drop(type_widget.value, name_widget.value)\n",
    "\n",
    "name_widget.observe(update)  \n",
    "#type_widget.observe(update)\n",
    "type_widget.observe(update2)\n",
    "\n",
    "\n",
    "\n",
    "interact(compare_results, \n",
    "        _type = type_widget,\n",
    "         name = name_widget,\n",
    "         subject =  course_widget,\n",
    "         name2 = name_widget2\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic. Now we can compare which schools do well and which do poorly and in what subject. I note that the first school/authority `name` is used to filter out subjects that they don't have data for. That means that you might not see all their choices if youre using them in `name2`. I also note that if a school/authority has no test scores, then it defauls to a blank grid for mathematics 9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makin' A Map\n",
    "\n",
    "NOTE: This section is time consuming and not that interesting yet, feel free not to run this entire section as it only adds a map . \n",
    "\n",
    "First, we need to get the GPS coordinates of all those schools into the data frames. We can do that first by downloading the location data from Alberta Ed and putting it into a separate data frame, which we do thusly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc =  pd.read_excel(\"https://education.alberta.ca/media/1626669/authority_and_school.xlsx\", skiprows=[0,1])\n",
    "df_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a new frame which contains the school name and the latitude and longitude coordinates using the `geocoder` python module. Note that this is rate limited as we're basically running a bunch of Google queries, and they tend to get unhappy if you do that too fast. So this can take a few minutes to complete. I also note that because this is \"big business\" you can only get 2500 locations a day from Google. So this is what I would consider \"open-ish\" data.\n",
    "\n",
    "NOTE: This is only for a map and it takes probably about an hour to gather all of the latitude and longitude coordinates of each school. So, if you don't want to gather that data, that's okay. However, pay attention to some merge notes of frames so you don't run into problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder \n",
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"School Name\"] = df_loc['School Name']\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_coords(postal_code):\n",
    "    count = 0\n",
    "    clear_output(wait=True)\n",
    "    print(\"Looking up coordinates...\")\n",
    "    \n",
    "    if postal_code is None:\n",
    "        return None, None\n",
    "    \n",
    "    while True:\n",
    "        # Need to slow down the loop so we don't get throttled by asking \n",
    "        # too many queries. Google also rate-limits you to 50 per second.\n",
    "        time.sleep(.1)\n",
    "        count += 1\n",
    "        \n",
    "        # If you want to save the data you MUST use the google\n",
    "        # geocoder. Otherwise you're violating TOS\n",
    "       #g = geocoder.google(postal_code)\n",
    "\n",
    "        g = geocoder.arcgis(postal_code)\n",
    "        \n",
    "        try:\n",
    "            to_return =  g.json['lat'], g.json['lng']\n",
    "            print(g.json['lat'], g.json['lng'])\n",
    "            if count > 1:\n",
    "                print(\"Finally grabbed\", postal_code, \"on try\", count)    \n",
    "            break \n",
    "       \n",
    "        except:\n",
    "            \n",
    "            print(\"tried\", postal_code, count, \"times\")\n",
    "            \n",
    "            if count > 25:\n",
    "                print (\"I don't think\", postal_code, \"exists -- trying approximate\",postal_code[0:3] )\n",
    "                postal_code = postal_code[0:3]\n",
    "           \n",
    "            if count > 50:\n",
    "                print (\"I don't think\", postal_code, \"exists or you've timed out\")\n",
    "                to_return = None, None \n",
    "                break\n",
    "            continue\n",
    "            \n",
    "    return to_return\n",
    "\n",
    "\n",
    "\n",
    "temp_df[\"PC\"] = df_loc['School Postal Code']\n",
    "# Here's we're applying our function on evry enery of the column\n",
    "temp_df[\"coords\"] = temp_df[\"PC\"].apply(get_coords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df[['lat', 'long']] = temp_df['coords'].apply(pd.Series)\n",
    "\n",
    "# temp_df = save.copy()\n",
    "\n",
    "#pd.save.to_csv(\"GPS_Coordinates.csv\")\n",
    "\n",
    "\n",
    "#save = temp_df.copy()\n",
    "temp_df.to_csv(\"Coordinates_and_colors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_districts = list(schools_reshaped[\"Authority Name\"].unique())\n",
    "len(list_of_districts)\n",
    "\n",
    "\n",
    "a = {}\n",
    "# create dictionary of schools - district\n",
    "for schools in schools_reshaped[\"School Name\"].unique():\n",
    "    try:\n",
    "        a[schools] = list(schools_reshaped[schools_reshaped[\"School Name\"] == schools][\"Authority Name\"])[0]\n",
    "    except:\n",
    "        print(list(schools_reshaped[schools_reshaped[\"School Name\"] == schools][\"Authority Name\"]), schools)\n",
    "\n",
    "\n",
    "districts = pd.DataFrame(list(a.items()), columns=['School Name', 'Authority Name'])\n",
    "# Create colour dictionary of all districts\n",
    "c = ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 360, len(list_of_districts))]\n",
    "\n",
    "b = {}\n",
    "\n",
    "for i, district in enumerate(list_of_districts):\n",
    "    b[district] = c[i]\n",
    "    \n",
    "for keys in a:\n",
    "    colour = b[a[keys]]\n",
    "    a[keys] = str(colour)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = pd.DataFrame(list(a.items()), columns=['School Name', 'Color'])\n",
    "\n",
    "\n",
    "temp_df = pd.merge(temp_df, colors,on='School Name', how='left')\n",
    "temp_df = pd.merge(temp_df, districts, on='School Name', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.graph_objs import *\n",
    "# Again... this is semi-open data. This is free you use, you just need to generate an API key from their website\n",
    "# YOu need to create an account and get an access key from mapbox.com\n",
    "mapbox_access_token = YOUR_API_KEY_HERE #\n",
    "\n",
    "# temp_df = pd.merge(temp_df,schools_reshaped, on=\"School Name\")\n",
    "\n",
    "data = Data([\n",
    "    Scattermapbox(\n",
    "        lat=temp_df['lat'],\n",
    "        lon=temp_df['long'],\n",
    "        mode='markers',\n",
    "        marker=Marker(\n",
    "            size=5,\n",
    "            color= temp_df[\"Color\"], \n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=temp_df['School Name']+ \"<br>\" + temp_df['Authority Name']\n",
    "  \n",
    "    ),\n",
    "    Scattermapbox(\n",
    "        lat=temp_df['lat'],\n",
    "        lon=temp_df['long'],\n",
    "        mode='markers',\n",
    "        marker=Marker(\n",
    "            size=8,\n",
    "            color= temp_df[\"Color\"],\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=temp_df['School Name'] + \"<br>\" + temp_df['Authority Name'],\n",
    "        hoverinfo= None #temp_df['School Name']\n",
    "    )]\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title='Alberta Schools',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    showlegend=False,\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        bearing=0,\n",
    "        center=dict(\n",
    "            lat=53.544389,\n",
    "            lon=-113.4909267\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=3.5,\n",
    "        style='light'\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig = { 'data':data, 'layout':layout }\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to scroll around on this map to your hearts content. Each marker represents the approximate location of a school and is colored according to which school district it falls under. Hover over them in order to see the location, school name, and district name. Some schools are plotted as simply black. This means that we didn't have enough data to properly label them and they're likely schools that don't report data all that often.\n",
    "\n",
    "NOTE: If you haven't created the map, the merge below won't work by default, see comments below for what to comment out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does Differences in Funding Affect Student Performance? \n",
    "\n",
    "The code below assumes you've downloaded all the PDFs off of the Alberta education site containing funding information from each district. If you don't have it you can either download those pdfs yourself (not recommended) or get them from our swift container `callysto-open-data` called `district_funding.csv`, alternatively you can download it [here](https://swift-yeg.cloud.cybera.ca:8080/v1/AUTH_233e84cd313945c992b4b585f7b9125d/callysto-open-data/district_funding.csv), but you'll have to move it into this directory. \n",
    "\n",
    "\n",
    "Most of the code below is just wrangling data and making plots of that data. However, what we're doing is gathering all our funding data, combining it with our data frames and then plotting it. What we'll then have is the performance of each district against the provincial average in terms of test scores for each ear and subject, as well as a graph of how those test scores were affected by differences in _total_ funding. In order to do that, we plot the density of funding and performance grades for the entire province, and then fit a line to it in order to judge positive/negative coorelation between funding and grade performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# If you don't have the LAT LONG data, uncomment the line below and run this cell .\n",
    "# temp_df = schools_reshaped.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import requests\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def convert(x):\n",
    "    try:\n",
    "        return x.astype(int)\n",
    "    except:\n",
    "        return x\n",
    "   \n",
    "def get_funding_data(paths = \"FundingPdf/*.pdf\"):\n",
    "    data = []\n",
    "    count = 0\n",
    "    for file in glob.iglob(paths):\n",
    "        parsedPDF = parser.from_file(file)\n",
    "    \n",
    "        name = file.split(\"/\")[-1]\n",
    "        name = name.replace(\".pdf\", \"\")\n",
    "        name = name.replace(\"-\", \" \").title()\n",
    "        name = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", name)\n",
    "        try:\n",
    "            name = name.replace(\" No \", \" No. \")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try: \n",
    "            name = name.replace(\" Ltd\", \" Ltd. \")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        name = name.strip()\n",
    "\n",
    "        estimated_funding = None\n",
    "        projected_funding =  None\n",
    "        estimated_enroll =  None\n",
    "        projected_enroll =  None\n",
    "        year = None\n",
    "    \n",
    "        for line in parsedPDF['content'].split('\\n')[::-1]:\n",
    "\n",
    "            if \"TOTAL FUNDING\" in line:\n",
    "                estimate_funding = line.split()[2].replace('$',\"\").replace(\",\",\"\")\n",
    "                projected_funding = line.split()[3].replace('$',\"\").replace(\",\",\"\")\n",
    "        \n",
    "            if \"As of \" in line:\n",
    "          \n",
    "                try:\n",
    "                    print(int(line.split()[-1]))\n",
    "                    year = line.split()[-1]\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "            if \"Funded Enrolment for Grades 1 - 12\" in line:\n",
    "           \n",
    "                estimated_enroll = line.split()[7].replace(\",\", \"\")\n",
    "                projected_enroll = line.split()[9].replace(\",\", \"\")\n",
    "            elif \"Enrolment for Grades 1 - 12\" in line:\n",
    "           \n",
    "                estimated_enroll = line.split()[6].replace(\",\", \"\")\n",
    "                projected_enroll = line.split()[8].replace(\",\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "        data.append([name, estimate_funding, projected_funding, estimated_enroll, projected_enroll, year])\n",
    "   \n",
    "\n",
    "    df = pd.DataFrame(data, columns = [\"Authority Name\", \"Estimated Funding\", \"Projected Funding\", \"Estimated 1-12\", \"Projected 1-12\",\"Year\"])\n",
    "    df.to_csv(\"district_funding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add district funding \n",
    "try: \n",
    "    funding = pd.read_csv(\"district_funding.csv\")\n",
    "    del funding[\"Unnamed: 0\"]\n",
    "except:\n",
    "    get_funding_data()\n",
    "    \n",
    "    \n",
    "\n",
    "funding[\"Year\"] = np.nan_to_num(funding[\"Year\"]).astype(int)\n",
    "funding[\"Estimated 1-12\"] = np.nan_to_num(funding[\"Estimated 1-12\"]).astype(int)\n",
    "funding[\"Projected 1-12\"] = np.nan_to_num(funding[\"Projected 1-12\"]).astype(int)\n",
    "# Don't need this year's data. \n",
    "funding = funding[funding.Year != 2018]\n",
    "#funding = funding[funding.Year != np.nan]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing = pd.merge()\n",
    "\n",
    "testing = pd.merge(temp_df, funding,how='left', on = [\"Authority Name\"])\n",
    "#\n",
    "# There's a panda's gotcha with NaN types in integer columns so we have to\n",
    "# go through all this crap to deal with it. \n",
    "testing[\"Year\"] = np.nan_to_num(testing[\"Year\"]).astype(int)\n",
    "testing[\"Year\"] = np.nan_to_num(testing[\"Year\"]).astype(str)\n",
    "testing[\"Year\"] = testing[\"Year\"].replace('0', Nan)\n",
    "combined_frame = pd.merge(schools_reshaped, testing, how='left', on=['Authority Name', \"School Name\", \"Year\"])\n",
    "\n",
    "# create funding per student. \n",
    "combined_frame[\"Est Fund Per Student\"] = combined_frame[\"Estimated Funding\"]/combined_frame[\"Estimated 1-12\"]\n",
    "combined_frame[\"Proj Fund Per Student\"] = combined_frame[\"Projected Funding\"]/combined_frame[\"Projected 1-12\"]\n",
    "\n",
    "\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_plot_frame = combined_frame.sort_values(\"Year\").copy()# .replace(0, np.NaN)\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "from matplotlib import animation\n",
    "import numpy.ma as ma\n",
    "from scipy.stats import mstats\n",
    "\n",
    "def reject_outliers(data, m = 2.):\n",
    "    d = np.abs(data - np.median(data))\n",
    "    mdev = np.median(d)\n",
    "    s = d/mdev if mdev else 0.\n",
    "    return data[s<m]\n",
    "\n",
    "\n",
    "years = list(density_plot_frame[\"Year\"].unique())\n",
    "def make_density(category, year, subject, Authority=False, filter=False):\n",
    "    YEARS = list(density_plot_frame[\"Year\"].unique())\n",
    "    if subject:\n",
    "        x = density_plot_frame[density_plot_frame[\"Course Name\"] == subject]\n",
    "        grade = x[[category, \"Year\"]]\n",
    "        funding = x[[\"Est Fund Per Student\", \"Year\"]]\n",
    "        points = x[[\"Est Fund Per Student\", \"Year\", category, \"Authority Name\"]]\n",
    "            \n",
    "    else:\n",
    "        grade = density_plot_frame[[category, \"Year\"]]\n",
    "        funding = density_plot_frame[[\"Est Fund Per Student\", \"Year\"]]\n",
    "        points = density_plot_frame[[\"Est Fund Per Student\", \"Year\", category,\"Authority Name\"]]\n",
    "    \n",
    "    if Authority:\n",
    "        f, ax = plt.subplots(figsize=(7, 7))\n",
    "        \n",
    "        for i, year in enumerate(YEARS):\n",
    "                line = points[points[\"Year\"] == year][category]\n",
    "                line = line.mean()\n",
    "                if i == 0:\n",
    "                    label = \"Provincial Mean\"\n",
    "                else:\n",
    "                    label = \"\"\n",
    "                plt.axhline(y=line,xmin= (i+.1)/(len(YEARS)), xmax = (i+1-.1)/(len(YEARS)), c=\"g\", label = label)\n",
    "       \n",
    "        plt.style.use('ggplot')\n",
    "        points = points[points[\"Authority Name\"] == Authority]\n",
    "        grade = grade.dropna()\n",
    "        dd = pd.melt(points[[\"Year\", category]], id_vars = [\"Year\"], var_name = [category])\n",
    "        title = ''.join([Authority, \"\\n\", subject])\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            sns.boxplot(x=\"Year\", y=\"value\", data=dd, hue=category)\n",
    "        except:\n",
    "            title = ''.join([Authority, \"\\n\", subject,\" No data\"])\n",
    "        plt.title(title)\n",
    "\n",
    "    else: \n",
    "        f, (ax1, ax2) = plt.subplots(2, figsize=(9, 9))\n",
    "        plt.tight_layout(pad=4)\n",
    "       # plt.subplot(2,1,1)\n",
    "        points = points.dropna()\n",
    "        if year:\n",
    "            x = points[points[\"Year\"] == year][category]\n",
    "            y = points[points[\"Year\"] == year][\"Est Fund Per Student\"]\n",
    "            if filter:\n",
    "                t_f = points[points[\"Year\"] == year][[category, \"Est Fund Per Student\"]]\n",
    "            \n",
    "        else:\n",
    "            x = points[category]\n",
    "            y = points[\"Est Fund Per Student\"]\n",
    "            if filter:\n",
    "                t_f = points[[category, \"Est Fund Per Student\"]]\n",
    "            \n",
    "\n",
    "        x1 = x.quantile(0.25)\n",
    "        x2 = x.quantile(0.75)\n",
    "        y1 = y.quantile(0.25)\n",
    "        y2 = y.quantile(0.75)\n",
    "        \n",
    "        ax1.plot([x1,x1], [y1,y2], c ='r', label = \"Box contains\\n50% of data\")\n",
    "        ax1.plot([x1,x2], [y1,y1], c='r')\n",
    "        ax1.plot([x1,x2], [y2,y2], c='r')\n",
    "        ax1.plot([x2,x2], [y1,y2], c='r')\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # To get an idea for the trend I\"m plotting al ine. \n",
    "            # That said these errors are likely VERY non guassian\n",
    "            # I don't feel like plotting them -- too deep in rabbit hole\n",
    "            # to go down another. SO keep in mind these are \"trends\"\n",
    "            # and shouldn't be read into beyond a positive/negative \n",
    "            # correlation. \n",
    "            \n",
    "            #np.linspace(0,100)\n",
    "            \n",
    "\n",
    "            \n",
    "            if filter:\n",
    "                # Filter outliers by one stadard dev. (VERY AGRESSIVE) \n",
    "                top1 = t_f[category].mean() + t_f[category].std()\n",
    "                top2 = t_f[\"Est Fund Per Student\"].mean() + t_f[\"Est Fund Per Student\"].std()\n",
    "                bottom1 = t_f[category].mean() - t_f[category].std()\n",
    "                bottom2 = t_f[\"Est Fund Per Student\"].mean() - t_f[\"Est Fund Per Student\"].std()\n",
    "                t_f = t_f[t_f[category] < top1]\n",
    "                t_f = t_f[t_f[category] > bottom1]\n",
    "                tf = t_f[t_f[\"Est Fund Per Student\"] < top2]\n",
    "                tf = t_f[t_f[\"Est Fund Per Student\"] > bottom2]\n",
    "\n",
    "                x = tf[category]\n",
    "                y = tf[\"Est Fund Per Student\"]\n",
    "           \n",
    "            limits = x\n",
    "            fit, V = np.polyfit(x, y, deg=1, cov=True)\n",
    "            \n",
    "            # 62 percentile. Though probably not really \n",
    "            # as this calculation requires the errors to be normally distributed.\n",
    "            error = 2*np.sqrt(np.diag(V))\n",
    "\n",
    "            label = ''.join([\"Line of best fit\\n\", \n",
    "                            str(round(fit[0],2)), \n",
    "                            \"±\",\n",
    "                            str(round(error[0])),\n",
    "                            \"x + \",\n",
    "                            str(round(fit[1],2)), \n",
    "                            \"±\",\n",
    "                            str(round(error[1],2))])\n",
    "            \n",
    "            ax1.plot(limits, fit[0] * limits + fit[1], \n",
    "                     color='purple', \n",
    "                     label = label)\n",
    "            ax1.plot(limits, \n",
    "                     (fit[0]+ error[0]) * limits + fit[1] + error[1], \n",
    "                     color = 'orange',\n",
    "                    label = \"\")\n",
    "            ax1.plot(limits, \n",
    "                     (fit[0]- error[0]) * limits + fit[1] - error[1],\n",
    "                     color = 'orange',\n",
    "                    label = \"\")\n",
    "            \n",
    "            test = fit[0] * x + fit[1]\n",
    "            residual = y - test\n",
    "            \n",
    "        # Naked exception because I'm a rule breaker. \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        if subject:\n",
    "            pass\n",
    "        else:\n",
    "            subject = \"All\"\n",
    "        \n",
    "        title = \"\".join([\"All Districts\" ,\n",
    "                 \"\\nMean Funding = $\", \n",
    "                 str(round(y.mean(),2)),\n",
    "                \"\\nMean Percent = \",\n",
    "                str(round(x.mean(),2)),\n",
    "                \" %\",\n",
    "                \"\\nSubject: \",\n",
    "                 subject])\n",
    "        \n",
    "        ax1.set_title(title)\n",
    "        ax1.legend()\n",
    "        \n",
    "        sns.kdeplot(x, y, shade=True, ax=ax1)\n",
    "        \n",
    "        ax2.set_title(\"Distribution of Residuals of Line of Best Fit\")\n",
    "        \n",
    "        # Test if the residual is normally distributed to judge our LOBF\n",
    "        z,pval = mstats.normaltest(residual)\n",
    "        if pval < 0.05:\n",
    "            text = \"Errors probably not normally distributed\\n(Line of best fit shows approximate correlation only)\" \n",
    "        else:\n",
    "            text = \"Errors probably normally distributed\\n(Line of best fit can be used to extrapolate)\"\n",
    "        \n",
    "        ax2.set_xlabel(\"Distance from LOBF\")\n",
    "        ax2.set_ylabel(\"Counts\")\n",
    "        ax2.hist(residual, bins = 20, histtype='bar', ec='black', label = text)\n",
    "        ax2.legend()\n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "# this is a lazy copy-pase reformat of my filter function. I should probably ahve\n",
    "# just written a better function originally .\n",
    "        \n",
    "def course_drop2(_type, name):\n",
    "    \n",
    "    courses = list(density_plot_frame['Course Name'].unique())\n",
    "    \n",
    "    if not name:\n",
    "        return courses\n",
    "    \n",
    "    filtered_course_list = []\n",
    "   \n",
    "    for course in courses:\n",
    "        result = pd.DataFrame()\n",
    "        y = pd.DataFrame()\n",
    "        result = density_plot_frame[density_plot_frame[_type] == name].copy()\n",
    "        result = result[result['Course Name'] == course].copy()\n",
    "        if _type == \"School Name\":\n",
    "            y = result['Sch % Acc of Writing']\n",
    "       \n",
    "        if _type == \"Authority Name\":\n",
    "            y = result[\"Sch Acc\"].copy()\n",
    "            \n",
    "        if y.isnull().sum() >= len(y) - 2 or y.empty == True:\n",
    "            continue\n",
    "        else:\n",
    "            # if something exists, we'll count it\n",
    "            filtered_course_list.append(course)\n",
    "            \n",
    "    if len(filtered_course_list) == 0:\n",
    "        # TODO: make an empty thing instead of pretending they do math\n",
    "        filtered_course_list.append(\"Mathematics 6\")\n",
    "    return filtered_course_list    \n",
    "        \n",
    "    \n",
    "categories = sorted([\"Sch % Exc of Writing\", \"Sch % Acc of Writing\", \"Sch % Below of Writing\"])\n",
    "Authority = [None] + sorted(map(str,list(density_plot_frame[\"Authority Name\"].unique())))\n",
    "\n",
    "auth_widget = Dropdown(options= Authority)\n",
    "sub_widget = Dropdown()\n",
    "    \n",
    "def update(*args):\n",
    "    sub_widget.options = course_drop2(\"Authority Name\", auth_widget.value)\n",
    "    \n",
    "auth_widget.observe(update)\n",
    "\n",
    "years = [None] + years\n",
    "\n",
    "interact(make_density, \n",
    "        category = categories,\n",
    "        year = years, \n",
    "        subject = sub_widget,\n",
    "        Authority = auth_widget)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the widget above you can look at the year to year and total performance of every school district as a function of funding in the top graph, and below is a histogram of the linear fit residuals. In the small chance those residuals are normally distributed, the line of best fit can be used for extrapolation. However, if they are not, the line of best fit -at best- represents approximate correlation between student performance and funding. \n",
    "\n",
    "By selecting an authority you can view the performance of that district year to year against the provincial mean as well. \n",
    "\n",
    "A few interesting things to point out about the funding graph however: Excellent and acceptable scores seem to be slightly negatively correlated with funding i.e. more funding seems to be related to worse grades in some cases. That said, correlation does not depend on causation, and there are significant outliers from the actual cluster that may be over weighting the outliers. You can aggressively remove outliers by clicking the filter button which removes all points (in $x$ and $y$) that are greater than one standard deviation away from the mean of the data. \n",
    "\n",
    "Regardless the trend is the same, and funding doesn't seem to really matter in terms of performance. If anything, more funding seems to imply that the students do worse. However, the uncertainty is so large, and the residuals are far from normal, so at best I will cautiously state that funding amount does not seem to affect overall performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
