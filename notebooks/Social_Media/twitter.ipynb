{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Twitter data using Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Tweepy to analyze Twitter data with Python\n",
    "import tweepy\n",
    "from string import punctuation\n",
    "## Import the Natural Language Toolkit\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Twitter Data using the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect Jupyter to your Crowdmark API key.\n",
    "# Please place your Crowdmark API key somewhere and link to it by adjusting the route below.\n",
    "# The API key allows the computer hosting your Jupyter notebook to programmatically access data from Crowdmark.\n",
    "with open(\"/home/jcollian/.twitter-keys\", 'r') as f:\n",
    "    Twitter_Keys = f.read()\n",
    "# apiKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Twitter_Keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hack: Define a dictionary with `keys = {...}` using the content from Twitter_Keys. The dictionary will populate your Twitter keys below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == OAuth Authentication ==\n",
    "#\n",
    "# This mode of authentication is the new preferred way\n",
    "# of authenticating with Twitter.\n",
    "# Source: https://github.com/tweepy/tweepy/blob/master/examples/oauth.py\n",
    "\n",
    "# The consumer keys can be found on your application's Details\n",
    "# page located at https://dev.twitter.com/apps (under \"OAuth settings\")\n",
    "\n",
    "consumer_key=keys[consumer_key]\n",
    "consumer_secret= keys[consumer_secret]\n",
    "\n",
    "# The access tokens can be found on your applications's Details\n",
    "# page located at https://dev.twitter.com/apps (located\n",
    "# under \"Your access token\")\n",
    "access_token=keys[access_token]\n",
    "access_token_secret=keys[access_token_secret]\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.secure = True\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# If the authentication was successful, you should\n",
    "# see the name of the account print out\n",
    "print(api.me().name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the Twitter Account to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenName = 'pimsmath'\n",
    "tweetsPerRetrieval = 50\n",
    "\n",
    "# return list of Status object instances\n",
    "data = api.user_timeline(\n",
    "    screen_name=screenName,\n",
    "    count=tweetsPerRetrieval,\n",
    "    tweet_mode='extended', # avoid truncation\n",
    "    include_rts=True # include retweets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect recent tweets\n",
    "tweets = [] + data # extend first tweets\n",
    "lastIndex = tweets[-1].id - 1 # get last tweet index\n",
    "\n",
    "# recover tweets\n",
    "while len(data) > 0:\n",
    "    data = api.user_timeline(\n",
    "        screen_name=screenName,\n",
    "        count=tweetsPerRetrieval,\n",
    "        max_id=lastIndex, # prevent duplicates\n",
    "        tweet_mode='extended',\n",
    "        include_rts=True,\n",
    "    )\n",
    "    tweets += data\n",
    "    lastIndex = tweets[-1].id - 1\n",
    "print('Retrieved %d tweets!' % len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate mentions, get names\n",
    "mentions = [m['screen_name']\n",
    "                for t in tweets\n",
    "                    for m in t.entities['user_mentions']]\n",
    "mentions[:5] # slice first five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate media, get each URL\n",
    "remedia = [m['media_url']\n",
    "               for t in tweets\n",
    "                   if hasattr(t, 'retweeted_status')\n",
    "                   and 'media' in t.retweeted_status.entities\n",
    "                       for m in t.retweeted_status.entities['media']]\n",
    "\n",
    "media = remedia + [m['media_url']\n",
    "                       for t in tweets\n",
    "                           if 'media' in t.entities \n",
    "                               for m in t.entities['media']]\n",
    "media[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefer retweet data, get tweet text\n",
    "text = [t.retweeted_status.full_text\n",
    "            if hasattr(t, 'retweeted_status')\n",
    "            else t.full_text\n",
    "                for t in tweets]\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize words\n",
    "tokens = []\n",
    "for t in text:\n",
    "    for w in t.replace('\\n', '').lower().split(' '): # remove newlines, split on spaces\n",
    "        try:\n",
    "            if (\n",
    "                not (w.startswith('http') or w.startswith('@')) # ignore if URL or mention\n",
    "                and w not in stopwords.words('english') # ignore stopwords\n",
    "                and str(w).translate(punctuation) != '' # ignore punctuation\n",
    "                and w != u''): # ignore empties\n",
    "                    tokens.append(str(w).translate(punctuation))\n",
    "        except UnicodeEncodeError: pass # ignore if not ascii\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word frequency\n",
    "freq = dict((t, tokens.count(t)) for t in tokens)\n",
    "sorted(freq, key=freq.get)[-20:] # slice frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
